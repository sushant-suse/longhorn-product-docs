= Important Notes for v1.7.0
:current-version: {page-component-version}

Please see https://github.com/longhorn/longhorn/releases/tag/v{patch-version}[here] for the full release note.

== Warning

The `longhorn-share-manager` v1.7.3 is impacted by a https://github.com/nfs-ganesha/nfs-ganesha/issues/1132[regression issue] in nfs-ganesha v6.0+, which causes unexpected "permission denied" errors when non-root users access RWX volumes. To resolve this issue, replace `longhorn-share-manager:v1.7.3` with the hotfixed image `longhorn-share-manager:v1.7.3-hotfix-1`.

You can apply the update in one of the following ways:

* **Helm or Deployment Manifest**:
  Update the `longhorn-share-manager` image from `v1.7.3` to `v1.7.3-hotfix-1`, then perform an upgrade.

* **Manual Update**:
  Edit the `longhorn-manager` DaemonSet's `spec.containers.command` field as shown below:

[,yaml]
----
  ...
    spec:
      containers:
      - command:
        - longhorn-manager
        - -d
        - daemon
        - --engine-image
        - longhornio/longhorn-engine:v1.7.3
        - --instance-manager-image
        - longhornio/longhorn-instance-manager:v1.7.3
        - --share-manager-image
        - longhornio/longhorn-share-manager:v1.7.3-hotfix-1
        - --backing-image-manager-image
        - longhornio/backing-image-manager:v1.7.3
        - --support-bundle-manager-image
        - longhornio/support-bundle-kit:v0.0.51
        - --manager-image
        - longhornio/longhorn-manager:v1.7.3
        - --service-account
        - longhorn-service-account
        - --upgrade-version-check
  ...
----

Then, the newly created RWX volumes will use the hotfixed image. For existing attached RWX volumes, you can detach and reattach them to apply the hotfix.

For more information, see Issue https://github.com/longhorn/longhorn/issues/10979[#10979] and https://github.com/longhorn/longhorn/issues/10621[#10621].

== Deprecation

=== Environment Check Script

The functionality of the https://github.com/longhorn/longhorn/blob/master/scripts/environment_check.sh[environment check script] (`environment_check.sh`) overlaps with that of the Longhorn CLI, which is available starting with v1.7.0. Because of this, the script is deprecated in v1.7.0 and is scheduled for removal in v1.8.0.

== General

=== Supported Kubernetes Versions

Please ensure your Kubernetes cluster is at least v1.21 before upgrading to {longhorn-product-name} v{patch-version} (you can replace v{patch-version} with your desired Longhorn version) because this is the minimum version {longhorn-product-name} v{patch-version} supports.

=== Pod Security Policies Disabled & Pod Security Admission Introduction

* Longhorn pods require privileged access to manage nodes' storage. In {longhorn-product-name} `v1.3.x` or older, {longhorn-product-name} was shipping some Pod Security Policies by default, (e.g., https://github.com/longhorn/longhorn/blob/4ba39a989b4b482d51fd4bc651f61f2b419428bd/chart/values.yaml#L260[link]).
However, Pod Security Policy has been deprecated since Kubernetes v1.21 and removed since Kubernetes v1.25, https://kubernetes.io/docs/concepts/security/pod-security-policy/[link].
Therefore, we stopped shipping the Pod Security Policies by default.
For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
 ** Helm installation method: set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
 ** Kubectl installation method: need to apply the https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml[podsecuritypolicy.yaml] manifest in addition to applying the `longhorn.yaml` manifests.
 ** Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
* As a replacement for Pod Security Policy, Kubernetes provides a new mechanism, https://kubernetes.io/docs/concepts/security/pod-security-admission/[Pod Security Admission].
If you enable the Pod Security Admission controller and change the default behavior to block privileged pods,
you must add the correct labels to the namespace where Longhorn pods run to allow Longhorn pods to start successfully
(because Longhorn pods require privileged access to manage storage).
For example, adding the following labels to the namespace that is running Longhorn pods:
  `yaml
  apiVersion: v1
  kind: Namespace
  metadata:
    name: longhorn-system
    labels:
      pod-security.kubernetes.io/enforce: privileged
      pod-security.kubernetes.io/enforce-version: latest
      pod-security.kubernetes.io/audit: privileged
      pod-security.kubernetes.io/audit-version: latest
      pod-security.kubernetes.io/warn: privileged
      pod-security.kubernetes.io/warn-version: latest
 	`

=== Command Line Tool

The Longhorn CLI (binary name: `longhornctl`), which is the official Longhorn command line tool, was introduced in v1.7.0. This tool interacts with {longhorn-product-name} by creating Kubernetes custom resources (CRs) and executing commands inside a dedicated pod for in-cluster and host operations. Usage scenarios include installation, operations such as exporting replicas, and troubleshooting. For more information, see xref:longhorn-system/system-access/longhorn-cli.adoc[Command Line Tool (longhornctl)].

=== Minimum XFS Filesystem Size

Recent versions of `xfsprogs` (including the version {longhorn-product-name} currently uses) _do not allow_ the creation of XFS
filesystems https://git.kernel.org/pub/scm/fs/xfs/xfsprogs-dev.git/commit/?id=6e0ed3d19c54603f0f7d628ea04b550151d8a262[smaller than 300
MiB].
{longhorn-product-name} v{patch-version} does not allow the following:

* CSI flow: Volume provisioning if `resources.requests.storage < 300 Mi` and the corresponding StorageClass has `fsType:
xfs`
* {longhorn-product-name} UI: `Create PV/PVC` with `File System: XFS` action to be completed on a volume that has `spec.size < 300 Mi`

However, {longhorn-product-name} still allows the listed actions when cloning or restoring volumes created with earlier {longhorn-product-name}
versions.

=== Longhorn PVC with Block Volume Mode

Starting with v1.6.0, {longhorn-product-name} is changing the default group ID of {longhorn-product-name} devices from `0` (root group) to `6` (typically associated with the "disk" group).
This change allows non-root containers to read or write to PVs using the *Block* volume mode. Note that {longhorn-product-name} still keeps the owner of the {longhorn-product-name} block devices as root.
As a result, if your pod has security context such that it runs as non-root user and is part of the group id 0, the pod will no longer be able to read or write to {longhorn-product-name} block volume mode PVC anymore.
This use case should be very rare because running as a non-root user with the root group does not make much sense.
More specifically, this example will not work anymore:

[subs="+attributes",yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-block-vol
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  storageClassName: longhorn
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: block-volume-test
  namespace: default
spec:
  securityContext:
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
    supplementalGroups:
    - 0
  containers:
    - name: block-volume-test
      image: ubuntu:20.04
      command: ["sleep", "360000"]
      imagePullPolicy: IfNotPresent
      volumeDevices:
        - devicePath: /dev/longhorn/testblk
          name: block-vol
  volumes:
    - name: block-vol
      persistentVolumeClaim:
        claimName: longhorn-block-vol
----

From this version, you need to add group id 6 to the security context or run container as root. For more information, see xref:volumes/pvc-ownership-and-permission.adoc[Longhorn PVC ownership and permission]

=== Container-Optimized OS Support

Starting with {longhorn-product-name} v1.7.0, {longhorn-product-name} supports Container-Optimized OS (COS), providing robust and efficient persistent storage solutions for Kubernetes clusters running on COS. For more information, see xref:installation-setup/os-distro/container-optimized-os.adoc[Container-Optimized OS (COS) Support].

=== Upgrade Check Events

{longhorn-product-name} performs a pre-upgrade check when upgrading with Helm or Rancher App Marketplace. If a check fails, the upgrade will stop, and the reason for the check's failure will be recorded in an event. For more detail, see xref:upgrades/longhorn-components/upgrade-longhorn-manager.adoc[Upgrading Longhorn Manager].

=== Manual Checks Before Upgrade

Automated checks are only performed on some upgrade paths, and the pre-upgrade checker may not cover some scenarios. Manual checks, performed using either `kubectl` or the UI, are recommended for these scenarios. You can take mitigating actions or defer the upgrade until issues are addressed.

* Ensure that all V2 Data Engine volumes are detached and the replicas are stopped. The V2 Data Engine currently does not support live upgrades.
* Avoid upgrading when volumes are in the "Faulted" status. If all the replicas are deemed unusable, they may be deleted, and data may be permanently lost (if no usable backups exist).
* Avoid upgrading if a failed BackingImage exists. For more information, see xref:snapshots-backups/backing-image-backups.adoc[Backing Image].
* It is recommended to create a xref:snapshots-backups/system-backups/create-system-backup.adoc[Longhorn system backup] before performing the upgrade. This ensures that all critical resources, such as volumes and backing images, are backed up and can be restored in case any issues arise.

== Resilience

=== RWX Volumes Fast Failover

RWX Volumes fast failover is introduced in {longhorn-product-name} v1.7.0 to improve resilience to share-manager pod failures. This failover mechanism quickly detects and responds to share-manager pod failures independently of the Kubernetes node failure sequence and timing. For details, see xref:high-availability/rwx-volume-fast-failover.adoc[RWX Volume Fast Failover].

NOTE: In rare circumstances, it is possible for the failover to become deadlocked. This happens if the NFS server pod creation is blocked by a recovery action that is itself blocked by the failover-in-process state.  If the feature is enabled, and a failover takes more than a minute or two, it is probably stuck in this situation.  There is an explanation and a workaround in xref:high-availability/rwx-volume-fast-failover.adoc[RWX Volume Fast Failover].

=== Timeout Configuration for Replica Rebuilding and Snapshot Cloning

Starting with v1.7.0, {longhorn-product-name} supports configuration of timeouts for replica rebuilding and snapshot cloning. Before v1.7.0, the replica rebuilding timeout was capped at 24 hours, which could cause failures for large volumes in slow bandwidth environments. The default timeout is still 24 hours but you can adjust it to accommodate different environments. For more information, see xref:longhorn-system/settings.adoc#_long_grpc_timeout[Long gRPC Timeout].

=== Change in Engine Replica Timeout Behavior

In versions earlier than v1.7.1, the xref:longhorn-system/settings.adoc#_long_grpc_timeout[Engine Replica Timeout] setting
was equally applied to all V1 volume replicas. In v1.7.1, a V1 engine marks the last active replica as failed only after
twice the configured number of seconds (timeout value x 2) have passed.

== Data Integrity and Reliability

=== Support Periodic and On-Demand Full Backups to Enhance Backup Reliability

Since {longhorn-product-name} v1.7.0, periodic and on-demand full backups have been supported to enhance backup reliability. Prior to v1.7.0, the initial backup was a full backup, with subsequent backups being incremental. If any block became corrupted, all backup revisions relying on that block would also be corrupted. To address this issue, {longhorn-product-name} now supports performing a full backup after every N incremental backups, as well as on-demand full backups. This approach decreases the likelihood of backup corruption and enhances the overall reliability of the backup process. For more information, see xref:snapshots-backups/volume-snapshots-backups/create-recurring-backup-snapshot-job.adoc[Recurring Snapshots and Backups] and xref:snapshots-backups/volume-snapshots-backups/create-backup.adoc[Create a Backup].

=== High Availability of Backing Images

To address the single point of failure (SPOF) issue with backing images, high availability for backing images was introduced in {longhorn-product-name} v1.7.0. For more information, please see xref:volumes/backing-images/backing-images.adoc#_number_of_copies[Backing Image].

== Scheduling

=== Auto-Balance Pressured Disks

The replica auto-balancing feature was enhanced in {longhorn-product-name} v1.7.0 to address disk space pressure from growing volumes. A new setting, called `replica-auto-balance-disk-pressure-percentage`, allows you to set a threshold for automatic actions. The enhancements reduce the need for manual intervention by automatically rebalancing replicas during disk pressure, and improve performance by enabling faster replica rebuilding using local file copying. For more information, see xref:longhorn-system/settings.adoc#replica-auto-balance-disk-pressure-threshold-[`replica-auto-balance-disk-pressure-percentage`] and https://github.com/longhorn/longhorn/issues/4105[Issue #_4_1_0_5].

== Networking

=== Storage Network Support for Read-Write-Many (RWX) Volumes

Starting with {longhorn-product-name} v1.7.0, the xref:longhorn-system/networking/storage-network.adoc[storage network] supports RWX volumes. However, the network's reliance on Multus results in a significant restriction.

Multus networks operate within the Kubernetes network namespace, so {longhorn-product-name} can mount NFS endpoints only within the CSI plugin pod container network namespace. Consequently, NFS mount connections to the Share Manager pod become unresponsive when the CSI plugin pod restarts. This occurs because the namespace in which the connection was established is no longer available.

{longhorn-product-name} circumvents this restriction by providing the following settings:

* xref:longhorn-system/settings.adoc#_storage_network_for_rwx_volume_enabled[Storage Network For RWX Volume Enabled]: When this setting is disabled, the storage network applies only to RWO volumes. The NFS client for RWX volumes is mounted over the cluster network in the host network namespace. This means that restarting the CSI plugin pod does not affect the NFS mount connections
* xref:longhorn-system/settings.adoc#_automatically_delete_workload_pod_when_the_volume_is_detached_unexpectedly[Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly]: When the RWX volumes are created over the storage network, this setting actively deletes RWX volume workload pods when the CSI plugin pod restarts. This allows the pods to be remounted and prevents dangling mount entries.

You can upgrade clusters with pre-existing RWX volume workloads to {longhorn-product-name} v1.7.0. During and after the upgrade, the workload pod must not be interrupted because the NFS share connection uses the cluster IP, which remains valid in the host network namespace.

To apply the storage network to existing RWX volumes, you must detach the volumes, enable the xref:longhorn-system/settings.adoc#_storage_network_for_rwx_volume_enabled[Storage Network For RWX Volume Enabled] setting, and then reattach the volumes.

For more information, see https://github.com/longhorn/longhorn/issues/8184[Issue #8184].

== Backup

=== Backup Data On The Remote Backup Server Might Be Deleted

{longhorn-product-name} may unintentionally delete backup-related custom resources (such as `BackupVolume`, `BackupBackingImage`, `SystemBackup`, and `Backup`) and backup data on the remote backup server before {longhorn-product-name} v{patch-version} in the following scenarios:

* An empty response from the NFS server due to server downtime.
* A race condition could delete the remote backup volume and its corresponding backups when the backup target is reset within a short period.

Starting with v{current-version}, {longhorn-product-name} handles backup-related custom resources in the following manner:

* If there are discrepancies between the backup information in the cluster and on the remote backup server, {longhorn-product-name} deletes only the backup-related custom resources in the cluster.
* The backup-related custom resources in the cluster may be deleted unintentionally while the remote backup data remains safely stored. The deleted resources are resynchronized from the remote backup server during the next polling period (if the backup target is available).

For more information, see https://github.com/longhorn/longhorn/issues/9530[Issue #9530].

== V2 Data Engine

=== Longhorn System Upgrade

{longhorn-product-name} currently does not support live upgrading of V2 volumes. Ensure that all V2 volumes are detached before initiating the upgrade process.

=== Enable Both `vfio_pci` and `uio_pci_generic` Kernel Modules

According to the https://spdk.io/doc/system_configuration.html[SPDK System Configuration User Guide], neither `vfio_pci` nor `uio_pci_generic` is universally suitable for all devices and environments. Therefore, users can enable both `vfio_pci` and `uio_pci_generic` kernel modules. This allows {longhorn-product-name} to automatically select the appropriate module. For more information, see this https://github.com/longhorn/longhorn/issues/9182[link].

=== Online Replica Rebuilding

Online replica rebuilding was introduced in {longhorn-product-name} 1.7.0, so offline replica rebuilding has been removed.

=== Block-type Disk Supports SPDK AIO, NVMe and VirtIO Bdev Drivers

Before {longhorn-product-name} v1.7.0, {longhorn-product-name} block-type disks only supported the SPDK AIO bdev driver, which introduced extra performance penalties. Since v1.7.0, block devices can be directly managed by SPDK NVMe or VirtIO bdev drivers, improving IO performance through a kernel bypass scheme. For more information, see this https://github.com/longhorn/longhorn/issues/7672[link].

=== Filesystem Trim

Filesystem trim is supported since Longhorn v1.7.0. If a disk is managed by the SPDK AIO bdev driver, the Trim (UNMAP) operation is not recommended in a production environment (ref). It is recommended to manage a block-type disk with an NVMe bdev driver.

=== Linux Kernel on Longhorn Nodes

Host machines with Linux kernel 5.15 may unexpectedly reboot when volume-related IO errors occur. To prevent this, update the Linux kernel on {longhorn-product-name} nodes to version 5.19 or later. For more information, see xref:longhorn-system/v2-data-engine/prerequisites.adoc[Prerequisites]. Version 6.7 or later is recommended for improved system stability.

=== Snapshot Creation Time As Shown in the UI Occasionally Changes

Snapshots created before {longhorn-product-name} v1.7.0 may change occasionally. This issue arises because the engine randomly selects a replica and its snapshot map each time the UI requests snapshot information or when a replica is rebuilt with a random healthy replica. This can lead to potential time gaps between snapshots among different replicas. Although this bug was fixed in v1.7.0, snapshots created before this version may still encounter the issue. For more information, see this https://github.com/longhorn/longhorn/issues/7641[link].

=== Unable To Revert a Volume to a Snapshot Created Before {longhorn-product-name} v1.7.0

Reverting a volume to a snapshot created before {longhorn-product-name} v1.7.0 is not supported due to an incorrect UserCreated flag set on the snapshot. The workaround is to back up the existing snapshots before upgrading to {longhorn-product-name} v1.7.0 and restore them if needed. The bug is fixed in v1.7.0, and more information can be found https://github.com/longhorn/longhorn/issues/9054[here].
