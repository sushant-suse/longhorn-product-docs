= Settings
:revdate: 2026-01-12
:page-revdate: {revdate}
:current-version: {page-component-version}

== Value Format Types by Supported Data Engines

Each setting supports only one of the following formats, based on its definition.  
The supported format determines which data engines can be configured and whether their values can differ.

* *Single value for all supported data engines*
** *Format:* Non-JSON string (for example, `1024`)
** The value applies to all supported data engines and must be the same across them.
** Data-engine-specific values are not allowed.

* *Data-engine-specific values for V1 and V2 data engines*
** *Format:* JSON object (for example, `{"v1": "value1", "v2": "value2"}`)
** Allows specifying different values for V1 and V2 data engines.

* *Data-engine-specific values for V1 data engine only*
** *Format:* JSON object with `v1` key only (for example, `{"v1": "value1"}`)
** Only the V1 data engine can be configured. The V2 data engine is not affected.

* *Data-engine-specific values for V2 data engine only*
** *Format:* JSON object with `v2` key only (for example, `{"v2": "value1"}`)
** Only the V2 data engine can be configured. The V1 data engine is not affected.

== Customizing Default Settings

To configure {longhorn-product-name} before installing it, see xref:longhorn-system/customize-default-settings.adoc[this section] for details.

== System Info

=== Default Engine Image

The default engine image is the image used by the manager and can be changed only when the manager starts.

Every {longhorn-product-name} release includes a new engine image. A green arrow appears on the Longhorn volumes not using the default engine, indicating that the volume needs to be upgraded.

=== Default Instance Manager Image

The default instance manager image is the image used by the manager and can be changed only when the manager starts.

=== Default Backing Image Manager Image

The default backing image manager image is the image used by the manager and can be changed only when the manager starts.

=== Support Bundle Manager Image

{longhorn-product-name} uses the support bundle manager image to generate support bundles.

A default image is provided during installation and upgrade, and you can also change this in the settings.

An example of the support bundle manager image:

*Default value*: `longhornio/support-bundle-kit:v0.0.14`

=== Current {longhorn-product-name} Version

The current {longhorn-product-name} version.

=== Latest {longhorn-product-name} Version

The latest available version of {longhorn-product-name}. This is automatically updated by the `Upgrade Checker`.

[NOTE]
====
This is only available if `Upgrade Checker` is enabled.
====

=== Stable {longhorn-product-name} Versions

The latest stable version of every minor release line. It is updated by the `Upgrade Checker` automatically.

== General

=== Node Drain Policy

*Default value*: `block-if-contains-last-replica`

Define the policy to use when a node with the last healthy replica of a volume is drained. Available options:

* `block-if-contains-last-replica`: {longhorn-product-name} blocks the drain when the node contains the last healthy replica of a
volume.
* `allow-if-replica-is-stopped`: {longhorn-product-name} allows the drain when the node contains the last healthy replica of a
volume but the replica is stopped.
+
[WARNING]
====
Possible data loss if the node is removed after draining.
====
+
* `always-allow`: {longhorn-product-name} automatically allows the drain even though the node contains the last healthy replica of a volume.
+
[WARNING]
====
Possible data loss if the node is removed after draining. Also possible data corruption if the last replica was running during the draining.
====
+
* `block-for-eviction`: {longhorn-product-name} automatically evicts all replicas and block the drain until eviction is complete.
+
[WARNING]
====
Can result in slow drains and extra data movement associated with replica rebuilding.
====
+
* `block-for-eviction-if-contains-last-replica`: {longhorn-product-name} automatically evicts any replicas that do not have a healthy counterpart and block the drain until eviction is complete.
+
[WARNING]
====
Can result in slow drains and extra data movement associated with replica rebuilding.
====

Each option has benefits and drawbacks. See xref:troubleshooting-maintenance/maintenance.adoc#_node_drain_policy_recommendations[Node Drain Policy Recommendations] for help deciding which is most appropriate in your environment.

=== Detach Manually Attached Volumes When Cordoned

*Default value*: `false`

{longhorn-product-name} automatically detaches volumes that are manually attached to the nodes which are cordoned.
This prevent the draining process stuck by the PDB of instance-manager which still has running engine on the node.

=== Automatically Clean up System Generated Snapshot

*Default value*: `true`

{longhorn-product-name} generates system snapshot during replica rebuild, and if a user does not setup a recurring snapshot schedule, all the system generated snapshots would be left in the replica, and user has to delete them manually, this setting allow {longhorn-product-name} to automatically cleanup system generated snapshot before and after replica rebuild.

=== Automatically Clean up Outdated Snapshots of Recurring Backup Jobs

*Default value*: `true`

If enabled, when running a recurring backup job, {longhorn-product-name} takes a new snapshot before creating the backup. {longhorn-product-name} retains only the snapshot used by the last backup job even if the value of the retain parameter is not 1.

If disabled, this setting ensures that the retained snapshots directly correspond to the backups on the remote backup target.

=== Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly

*Default value*: `true`

If enabled, {longhorn-product-name} automatically deletes the workload pod that is managed by a controller (for example, deployment, statefulset, daemonset, etc.) when Longhorn volume is detached unexpectedly (for example, during Kubernetes upgrade, Docker reboot, or network disconnect).
By deleting the pod, its controller restarts the pod and Kubernetes handles volume reattachment and remount.

If disabled, {longhorn-product-name} does not delete the workload pod that is managed by a controller. You need to manually restart the pod to reattach and remount the volume.

[NOTE]
====
* This setting does not apply to workload pods without a controller. {longhorn-product-name} never deletes such pods.
* Workload pods with *cluster network* RWX volumes. The setting does not apply to such pods because the Longhorn Share Manager, which provides the RWX NFS service, has its own resilience mechanism. This mechanism ensures availability until the volume is reattached without relying on the pod lifecycle to trigger volume reattachment. The setting does apply, however, to workload pods with *endpoint network* RWX volumes. For more information, see xref:volumes/rwx-volumes.adoc[ReadWriteMany (RWX) Volume] and xref:longhorn-system/networking/storage-network.adoc#limitation[Storage Network].
====

=== Blacklist for Automatic Workload Pod Deletion on Unexpected Volume Detachment

*Default value*: `""`

Blacklist of controller `api/kind` values for the setting <<_automatically_delete_workload_pod_when_the_volume_is_detached_unexpectedly,Automatically Delete Workload Pod when the Volume Is Detached Unexpectedly>>. If a workload pod is managed by a controller whose `api/kind` is listed in this blacklist, {longhorn-product-name} will not automatically delete the pod when its volume is unexpectedly detached. Multiple controller `api/kind` entries can be specified, separated by semicolons. For example: `apps/v1/StatefulSet;apps/v1/DaemonSet`.

[NOTE]
====
The controller `api/kind` is case sensitive and must exactly match the `api/kind` in the workload pod's owner reference.
====

=== Automatic Salvage

*Default value*: `true`

If enabled, volumes are automatically salvaged when all the replicas become faulty, for example, due to network disconnection. {longhorn-product-name} tries to figure out which replica(s) are usable, then use them for the volume.

=== Concurrent Automatic Engine Upgrade Per Node Limit

*Default value*: `0`

This setting controls how {longhorn-product-name} automatically upgrades volumes' engines to the new default engine image after upgrading Longhorn manager.

The value of this setting specifies the maximum number of engines per node that are allowed to upgrade to the default engine image at the same time.

If the value is `0`, {longhorn-product-name} does not automatically upgrade volume engines to the default version.

=== Concurrent Volume Backup Restore Per Node Limit

*Default value*: `5`

This setting controls how many volumes on a node can restore the backup concurrently.

{longhorn-product-name} blocks the backup restore once the restoring volume count exceeds the limit.

Set the value to *0* to disable backup restore.

=== Create Default Disk on Labeled Nodes

*Default value*: `false`

If no other disks exist, {longhorn-product-name} creates a default disk automatically, but only on nodes with the Kubernetes label `node.longhorn.io/create-default-disk=true`.

If this setting is disabled, the default disk is created on all new nodes when the node is first detected.

Use this option to scale the cluster without using storage on new nodes, or to customize disks for Longhorn nodes (xref:nodes/default-disk-and-node-config.adoc[see documentation]).

=== Custom Resource API Version

*Default value*: `longhorn.io/v1beta2`

The current customer resource's API version, for example, `longhorn.io/v1beta2`. Set by manager automatically.

=== Default Data Locality

*Default value*: `disabled`

A {longhorn-product-name} volume has data locality when at least one replica resides on the same node as the pod using the volume.  

This setting defines the default data locality for volumes created through the {longhorn-product-name} UI.  
For Kubernetes deployments, configure `dataLocality` in the StorageClass.

The available modes are:

* `disabled` (default): A replica may or may not reside on the same node as the attached volume or workload.
* `best-effort`: {longhorn-product-name} attempts to keep a replica on the same node as the attached volume or workload. The volume continues to operate even if environment constraints prevent local placement, such as insufficient disk space or incompatible disk tags.
* `strict-local`: {longhorn-product-name} enforces placement of a single replica on the same node as the attached volume. This mode provides higher IOPS and lower latency.

=== Default Data Path

*Default value*: `/var/lib/longhorn/`

Default path to use for storing data on a host.

Can be used with `Create Default Disk on Labeled Nodes` option, to make {longhorn-product-name} only use the nodes with specific storage mounted at, for example, `/opt/longhorn` when scaling the cluster.

=== Default {longhorn-product-name} Static StorageClass Name

*Default value*: `longhorn-static`

The `storageClassName` is used for persistent volumes (PVs) and persistent volume claims (PVCs) that reference an existing {longhorn-product-name} volume. You do not need to create a corresponding StorageClass object for this purpose because it is used only for label matching during PVC binding. The value must match an existing StorageClass. If the StorageClass `longhorn-static` does not already exist, {longhorn-product-name} creates it automatically. The default value is `longhorn-static`.

=== Default Replica Count

*Default value*: `{"v1":"3","v2":"3"}`

The default number of replicas when creating the volume from {longhorn-product-name} UI. For Kubernetes, update the `numberOfReplicas` in the StorageClass

The recommended way of choosing the default replica count is: if you have three or more nodes for storage, use 3; otherwise use 2. Using a single replica on a single node cluster is also OK, but the high availability functionality would not be available. You can still take snapshots or backups of the volume.

=== Deleting Confirmation Flag

*Default value*: `false`

This flag is designed to prevent {longhorn-product-name} from being accidentally uninstalled which leads to data loss.

* Set this flag to *true* to allow {longhorn-product-name} uninstallation.
* If this flag *false*, {longhorn-product-name} uninstallation job fails.

=== Disable Revision Counter

*Default value*: `{"v1":"true"}`

Allows engine controller and engine replica to disable revision counter file update for every data write. This improves the data path performance. See xref:high-availability/revision_counter.adoc[Revision Counter] for details.

=== Enable Upgrade Checker

*Default value*: `true`

Upgrade Checker checks for a new {longhorn-product-name} version periodically. When there is a new version available, it notifies the user in the {longhorn-product-name} UI.

=== Upgrade Responder URL

*Default value*: `pass:[https://longhorn-upgrade-responder.rancher.io/v1/checkupgrade]`

The Upgrade Responder sends a notification whenever a new {longhorn-product-name} version that you can upgrade to becomes available.

=== Allow Collecting {longhorn-product-name} Usage Metrics

*Default value*: `true`

When enabled, this setting allows {longhorn-product-name} to send anonymous usage metrics to https://metrics.longhorn.io/.

These metrics help provide insight into how {longhorn-product-name} is used and support future product improvements.

*Node Information collected from all cluster nodes includes:*

* Number of disks of each device type (HDD, SSD, NVMe, unknown).
+
This value may not be accurate for virtual machines.

* Number of disks for each Longhorn disk type (block, file system).
* Host system architecture.
* Host kernel release.
* Host operating system (OS) distribution.
* Kubernetes node provider.

*Cluster Information collected from one of the cluster nodes includes:*

* Longhorn namespace UID.
* Number of Longhorn nodes.
* Number of volumes of each access mode (RWO, RWX, unknown).
* Number of volumes of each data engine (v1, v2).
* Number of volumes of each data locality type (disabled, best_effort, strict_local, unknown).
* Number of volumes that are encrypted or unencrypted.
* Number of volumes of each frontend type (blockdev, iscsi).
* Number of replicas.
* Number of snapshots.
* Number of backing images.
* Number of orphans.
* Average volume size in bytes.
* Average volume actual size in bytes.
* Average number of snapshots per volume.
* Average number of replicas per volume.
* Average {longhorn-product-name} component CPU usage (instance manager, manager) in millicores.
* Average {longhorn-product-name} component memory usage (instance manager, manager) in bytes.
* Longhorn settings:
 ** Partially included:
  *** Backup Target Type or Protocol (azblob, cifs, nfs, s3, none, unknown). This is from the Backup Target setting.
 ** Included as true or false to indicate if this setting is configured:
  *** Priority Class
  *** Registry Secret
  *** Snapshot Data Integrity CronJob
  *** Storage Network
  *** Endpoint Network For RWX Volume
  *** System Managed Components Node Selector
  *** Taint Toleration
 ** Included as it is:
  *** Allow Recurring Job While Volume Is Detached
  *** Allow Volume Creation With Degraded Availability
  *** Automatically Clean up System Generated Snapshot
  *** Automatically Clean up Outdated Snapshots of Recurring Backup Jobs
  *** Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly
  *** Automatic Salvage
  *** Backing Image Cleanup Wait Interval
  *** Backing Image Recovery Wait Interval
  *** Backup Compression Method
  *** Backupstore Poll Interval
  *** Backup Concurrent Limit
  *** Concurrent Automatic Engine Upgrade Per Node Limit
  *** Concurrent Backup Restore Per Node Limit
  *** Concurrent Replica Rebuild Per Node Limit
  *** CRD API Version
  *** Create Default Disk Labeled Nodes
  *** Default Data Locality
  *** Default Replica Count
  *** Disable Revision Counter
  *** Disable Scheduling On Cordoned Node
  *** Engine Replica Timeout
  *** Failed Backup TTL
  *** Fast Replica Rebuild Enabled
  *** Guaranteed Instance Manager CPU
  *** Kubernetes Cluster Autoscaler Enabled
  *** Node Down Pod Deletion Policy
  *** Node Drain Policy
  *** Orphan Auto Deletion
  *** Recurring Failed Jobs History Limit
  *** Recurring Successful Jobs History Limit
  *** Remove Snapshots During Filesystem Trim
  *** Replica Auto Balance
  *** Replica File Sync HTTP Client Timeout
  *** Replica Replenishment Wait Interval
  *** Replica Soft Anti Affinity
  *** Replica Zone Soft Anti Affinity
  *** Replica Disk Soft Anti Affinity
  *** Restore Concurrent Limit
  *** Restore Volume Recurring Jobs
  *** Snapshot Data Integrity
  *** Snapshot DataIntegrity Immediate Check After Snapshot Creation
  *** Storage Minimal Available Percentage
  *** Storage Network For RWX Volume Enabled
  *** Storage Over Provisioning Percentage
  *** Storage Reserved Percentage For Default Disk
  *** Support Bundle Failed History Limit
  *** Support Bundle Node Collection Timeout
  *** System Managed Pods Image Pull Policy

The `Upgrade Checker` needs to be enabled to periodically send the collected data.

=== Pod Deletion Policy When Node is Down

*Default value*: `do-nothing`

Defines the {longhorn-product-name} action when a Volume is stuck with a StatefulSet or Deployment Pod on a node that is down.

* `do-nothing` is the default Kubernetes behavior of never force deleting StatefulSet or Deployment terminating pods. Since the pod on the node that is down is not removed, Longhorn volumes are stuck on nodes that are down.
* `delete-statefulset-pod` {longhorn-product-name} force delete StatefulSet terminating pods on nodes that are down to release Longhorn volumes so that Kubernetes can spin up replacement pods.
* `delete-deployment-pod` {longhorn-product-name} force delete Deployment terminating pods on nodes that are down to release Longhorn volumes so that Kubernetes can spin up replacement pods.
* `delete-both-statefulset-and-deployment-pod` {longhorn-product-name} force delete StatefulSet or Deployment terminating pods on nodes that are down to release Longhorn volumes so that Kubernetes can spin up replacement pods.

=== Registry Secret

The Kubernetes Secret name.

=== Replica Replenishment Wait Interval

*Default value*: `600`

When there is at least one failed replica volume in a degraded volume, this interval in seconds determines how long {longhorn-product-name} will wait at most in order to reuse the existing data of the failed replicas rather than directly creating a new replica for this volume.

WARNING: This wait interval works only when there is at least one failed replica in the volume. And this option may block the rebuilding for a while.

=== System Managed Pod Image Pull Policy

*Default value*: `if-not-present`

This setting defines the Image Pull Policy of Longhorn system managed pods, for example, instance manager, engine image, CSI driver, etc.

Notice that the new Image Pull Policy are only applied after the system managed pods restart.

This setting definition is exactly the same as that of in Kubernetes. Here are the available options:

* `always`. Every time the `kubelet` launches a container, the `kubelet` queries the container image registry to resolve the name to an image digest. If the `kubelet` has a container image with that exact digest cached locally, the `kubelet` uses its cached image; otherwise, the `kubelet` downloads (pulls) the image with the resolved digest, and uses that image to launch the container.
* `if-not-present`. The image is pulled only if it is not already present locally.
* `never`. The image is assumed to exist locally. No attempt is made to pull the image.

=== Backing Image Cleanup Wait Interval

*Default value*: `60`

This interval, in minutes, defines how long {longhorn-product-name} waits before cleaning up a backing image file when no replicas on the disk are using it.

=== Backing Image Recovery Wait Interval

*Default value*: `300`

This interval, in seconds, defines how long {longhorn-product-name} waits before re-downloading the backing image file after all disk files for that image enter the `failed` or `unknown` state.

[NOTE]
====
* This recovery only works for the backing image of which the creation type is `download`.
* File state `unknown` means the related manager pods on the pod is not running or the node itself is down or disconnected.
====

=== Default Min Number Of Backing Image Copies

*Default value*: `1`

The default minimum number of backing image copies {longhorn-product-name} maintains.

=== Engine Replica Timeout

*Default value*: `{"v1":"8","v2":"8"}`

Number of seconds a V1 Data Engine waits for a replica to respond before marking it as failed. Values between 8 and 30 are allowed. This setting takes effect only when there are outstanding input or output requests.

This setting only applies to additional replicas. A V1 engine marks the last active replica as failed only after twice the configured number of seconds (timeout value x 2) have passed. This behavior is intended to balance volume responsiveness with volume availability.

The engine can quickly (after the configured timeout) ignore individual replicas that become unresponsive in favor of other available ones. This ensures future input or output are not be held up.

The engine waits on the last replica (until twice the configured timeout) to prevent unnecessarily crashing as a result of having no available backends.

=== Support Bundle Failed History Limit

*Default value*: `1`

This setting specifies how many failed support bundles can exist in the cluster.

The retained failed support bundle is for analysis purposes and needs to clean up manually.

{longhorn-product-name} blocks support bundle creation when reaching the upper bound of the limitation. You can set this value to *0* to have {longhorn-product-name} automatically purge all failed support bundles.

=== Support Bundle Node Collection Timeout

*Default value*: `30`

Number of minutes {longhorn-product-name} allows for collection of node information and node logs for the support bundle.

If the collection process is not completed within the allotted time, {longhorn-product-name} continues generating the support bundle without the uncollected node data.

=== Fast Replica Rebuild Enabled

*Default value*: `{"v1":"true","v2":"true"}`

The setting enables fast replica rebuilding feature. It relies on the checksums of snapshot disk files, so setting the snapshot-data-integrity to *enable* or *fast-check* is a prerequisite.

=== Timeout of HTTP Client to Replica File Sync Server

*Default value*: `30`

The value in seconds specifies the timeout of the HTTP client to the replica's file sync server used for replica rebuilding, volume cloning, snapshot cloning, etc.

=== Offline Replica Rebuilding

*Default value*: `false`

Controls whether {longhorn-product-name} automatically rebuilds degraded replicas while the volume is detached. This setting only takes effect if the volume-level setting is set to `ignored` or `enabled`.

Available options:

* `true`: Enables offline replica rebuilding for all detached volumes (unless overridden at the volume level).
* `false`: Disables offline replica rebuilding globally (unless overridden at the volume level).

[NOTE]
====
Offline rebuilding occurs only when a volume is detached. Volumes in a faulted state are not trigger offline rebuilding.
====

This setting allows {longhorn-product-name} to automatically rebuild replicas for detached volumes when needed.

=== Long gRPC Timeout

*Default value*: `86400`

Number of seconds that {longhorn-product-name} allows for the completion of replica rebuilding and snapshot cloning operations.

=== RWX Volume Fast Failover (Experimental)

*Default value*: `false`

Enable improved ReadWriteMany volume HA by shortening the time it takes to recover from a node failure.

=== Log Level

*Default value*: `Log Level`

Longhorn Manager uses the following log levels: `Panic`, `Fatal`, `Error`, `Warn`, `Info`, `Debug`, and `Trace`. The default log level is `Info`.

=== Log Path

*Default value*: `/var/lib/longhorn/logs/`

This setting specifies the directory on the host where {longhorn-product-name} stores log files for the instance manager pod. Currently, this is only used for the instance manager pods in the v2 data engine.

=== Data Engine Log Level

*Default value*: `{"v2":"Notice"}`

Applies only to the V2 Data Engine. Specifies the log level for the Storage Performance Development Kit (SPDK) target daemon. Supported values: `Error`, `Warning`, `Notice`, `Info`, and `Debug`.

=== Data Engine Log Flags

*Default value*: `{"v2":""}`

Applies only to the V2 Data Engine. Specifies the log flags for the Storage Performance Development Kit (SPDK) target daemon.

=== Replica Rebuilding Bandwidth Limit

*Default value*: `{"v2":"0"}`

Applies only to the V2 Data Engine. Specifies the default write bandwidth limit, in megabytes per second (MB/s), for volume replica rebuilding.

=== Manager URL

*Default value*: `""`

*Example*: `pass:[https://longhorn.example.com]` or `pass:[https://longhorn.example.com:8443]`

The external URL to access the Longhorn Manager API. When configured, this URL is used to generate `actions` and `links` fields in API responses instead of deriving them from request headers or using internal pod IPs.

This setting is useful when accessing the Longhorn API through Ingress or Gateway API HTTPRoute, where the API may return internal cluster IPs if the ingress controller does not properly set `X-Forwarded-*` headers.

*Format*: `scheme://host[:port]` where:

* `scheme`: Must be `http` or `https`
* `host`: External hostname or IP address
* `port`: Optional port number (defaults to 80 for http, 443 for https)

*Requirements*:

* A URL must not contain a path, query parameters, or fragments
* IPv6 addresses must be enclosed in brackets (for example, `pass:[http://[2001:db8::1]:9500]`)

*When to use*:

* Access the Longhorn UI or API through Ingress with an external URL
* Use Gateway API HTTPRoute for external access
* API clients receive internal IPs in response URLs

*When empty* (default): URLs are constructed from HTTP request headers (`X-Forwarded-*`) or fall back to the request host.

For more details, see xref:important-notes.adoc#_manager_url_for_external_api_access[Manager URL for External API Access].

=== Default Ublk Queue Depth

*Default value*: `{"v2":"128"}`

The default depth of each queue for Ublk front-end. This setting applies to volumes using the V2 Data Engine with Ublk front-end.

=== Default Ublk Number Of Queue

*Default value*: `{"v2":"1"}`

The default the number of queues for ublk front-end. This setting applies to volumes using the V2 Data Engine with Ublk front-end.

=== Node Disk Health Monitoring

*Default value*: `true`

This setting controls whether {longhorn-product-name} monitors and records health information for node disks. When this setting is disabled, disk health checks and status updates are skipped.

== Snapshot

=== Snapshot Data Integrity

*Default value*: `{"v1":"fast-check","v2":"fast-check"}`

This setting allows users to enable or disable snapshot hashing and data integrity checking. Available options are:

* *disabled*: Disables snapshot disk file hashing and data integrity checks.
* *enabled*: Enables periodic snapshot disk file hashing and full data integrity checks. The {longhorn-product-name} system periodically hashes snapshot disk files to detect file system-unaware corruption, such as bit rot. These checks can affect system performance during each scan.
* *fast-check*: Enables snapshot disk file hashing with fast data integrity checks. In this mode, the system hashes snapshot disk files only if they have not been hashed before or if their modification time has changed. File system-unaware corruption cannot be detected in this mode, but the performance impact is reduced.

=== Immediate Snapshot Data Integrity Check After Creating a Snapshot

*Default value*: `{"v1":"false","v2":"false"}`

Hashing snapshot disk files impacts the performance of the system. The immediate snapshot hashing and checking can be disabled to minimize the impact after creating a snapshot.

=== Snapshot Data Integrity Check CronJob

*Default value*: `{"v1":"0 0 */7 * *","v2":"0 0 */7 * *"}`

Unix-cron string format. The setting specifies when {longhorn-product-name} checks the data integrity of snapshot disk files.

[WARNING]
====
Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
====

=== Snapshot Maximum Count

*Default value*: `250`

Maximum snapshot count for a volume. The value should be between 2 to 250.

=== Freeze Filesystem For Snapshot

*Default value*: `{"v1":"false"}`

This setting applies only to volumes that use the Kubernetes volume mode `Filesystem`.  
When enabled, {longhorn-product-name} freezes the file system of the volume immediately before creating a user-initiated snapshot.  
When disabled, or when the volume mode is `Block`, {longhorn-product-name} performs a system sync before creating a user-initiated snapshot.

Snapshots created with file system freezing enabled are more likely to be consistent because the file system is in a stable state at the time of snapshot creation. However, under heavy input or output, freezing the file system can take noticeable time and may briefly pause workload activity.

When this setting is disabled, {longhorn-product-name} flushes data to disk before creating the snapshot, but it cannot fully block writes between the system sync and snapshot creation.  
Workloads typically do not notice the snapshot operation because input and output continue during the sync.

The default value is `false`. Kernels `v5.17` and earlier may not recover correctly if a volume crashes during an active file system freeze. In this situation, the kernel might prevent unmounting the file system or stopping processes that use it until you reboot the node. Enable this setting only when using kernel version `5.17` or later and ext4 or XFS filesystems.

You can override this setting per volume by using the `freezeFilesystemForSnapshot` field in the {longhorn-product-name} UI, a StorageClass, or by editing an existing volume.  
`freezeFilesystemForSnapshot` accepts the following values:

*Default value*: `ignored`

* `ignored`: Instructs {longhorn-product-name} to use the global setting. This is the default option.
* `enabled`: Enables freezing before snapshots, regardless of the global setting.
* `disabled`: Disables freezing before snapshots, regardless of the global setting.

== Orphan

=== Orphaned Resource Automatic Deletion

*Example*: `replica-data;instance`

This setting allows {longhorn-product-name} to automatically delete `orphan` resources, which are typically Custom Resources (CRs) created by {longhorn-product-name} to represent detected orphaned entities. The deletion of an `orphan` CR subsequently triggers the cleanup of the actual orphaned data or runtime instance it represents. However, `orphan` resources associated with nodes that are in a `down` or `unknown` state are not be automatically cleaned up by this setting.

You can list the resource types to be automatically deleted as a semicolon-separated string. Available types include:

* `replica-data`: Represents replica data store.
* `instance`: Represents engine and replica runtime instance.

=== Orphaned Resource Automatic Deletion Grace Period

*Default value*: `300` seconds

Number of seconds {longhorn-product-name} waits before automatically deleting an orphaned custom resource (CR) and the actual orphaned data or runtime instance it represents.

[NOTE]
====
The grace period does not take effect when you manually delete an orphaned CR.
====

== Backups

=== Allow Recurring Job While Volume Is Detached

*Default value*: `false`

If this setting is enabled, {longhorn-product-name} automatically attaches the volume and takes snapshot or backup when it is the time to do recurring snapshot or backup.

[NOTE]
====
During the time the volume was attached automatically, the volume is not ready for the workload. The workload have to wait until the recurring job finishes.
====

==== Backup Execution Timeout

*Default value*: `1`

Number of minutes that {longhorn-product-name} allows for the backup execution.

=== Failed Backup Time To Live

*Default value*: `1440`

The interval in minutes to keep the backup resource that was failed. Set to 0 to disable the auto-deletion.

Failed backups are checked and cleaned up during backupstore polling which is controlled by *Backupstore Poll Interval* setting. Hence this value determines the minimal wait interval of the cleanup. And the actual cleanup interval is multiple of *Backupstore Poll Interval*. Disabling *Backupstore Poll Interval* also means to disable failed backup auto-deletion.

=== Cronjob Failed Jobs History Limit

*Default value*: `1`

This setting specifies how many failed backup or snapshot job histories should be retained.

History is not retained if the value is 0.

=== Cronjob Successful Jobs History Limit

*Default value*: `1`

This setting specifies how many successful backup or snapshot job histories should be retained.

History is not be retained if the value is 0.

=== Restore Volume Recurring Jobs

*Default value*: `false`

This setting allows restoring the recurring jobs of a backup volume from the backup target during a volume restoration if they do not exist on the cluster.
This is also a volume-specific setting with the below options. Users can customize it for each volume to override the global setting.

*Default value*: `ignored`

* `ignored`: This is the default option that instructs {longhorn-product-name} to inherit from the global setting.
* `enabled`: This option instructs {longhorn-product-name} to restore volume recurring jobs or groups from the backup target forcibly.
* `disabled`: This option instructs {longhorn-product-name} no restoring volume recurring jobs or groups should be done.

=== Backup Compression Method

*Default value*: `lz4`

This setting allows users to specify backup compression method.

* `none`: Disable the compression method. Suitable for multimedia data such as encoded images and videos.
* `lz4`: Fast compression method. Suitable for flat files.
* `gzip`: A bit of higher compression ratio but slow.

=== Backup Concurrent Limit Per Backup

*Default value*: `2`

This setting controls how many worker threads per backup concurrently.

=== Restore Concurrent Limit Per Backup

*Default value*: `2`

This setting controls how many worker threads per restore concurrently.

=== Default Backup Block Size

*Default value*: `2`

Specifies the default backup block size (in MiB), used when creating a new volume. Supported values are `2` or `16`.

== Scheduling

=== Allow Volume Creation with Degraded Availability

*Default value*: `true`

This setting allows user to create and attach a volume that does not have all the replicas scheduled at the time of creation.

NOTE: It is recommended to disable this setting when using {longhorn-product-name} in the production environment. See xref:installation-setup/best-practices.adoc[Best Practices] for details.

=== Disable Scheduling On Cordoned Node

*Default value*: `true`

When this setting is checked, the Longhorn Manager does not allow scheduling replicas on Kubernetes cordoned nodes.

When this setting is un-checked, the Longhorn Manager allows scheduling replicas on Kubernetes cordoned nodes.

=== Replica Node Level Soft Anti-Affinity

*Default value*: `false`

When this setting is checked, the Longhorn Manager allows scheduling on nodes with existing healthy replicas of the same volume.

When this setting is un-checked, Longhorn Manager prevents scheduling on nodes with existing healthy replicas of the same volume.

[NOTE]
====
* This setting is superseded if replicas are forbidden to share a zone by the Replica Zone Level Anti-Affinity setting.
====

=== Replica Zone Level Soft Anti-Affinity

*Default value*: `true`

When this setting is checked, the Longhorn Manager allows scheduling new replicas of a volume to the nodes in the same zone as existing healthy replicas.

When this setting is un-checked, Longhorn Manager prevents scheduling new replicas of a volume to the nodes in the same zone as existing healthy replicas.

[NOTE]
====
* Nodes that do not belong to any zone are treated as if they belong to the same zone.
* {longhorn-product-name} relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
====

=== Replica Disk Level Soft Anti-Affinity

*Default value*: `true`

When this setting is checked, the Longhorn Manager allows scheduling new replicas of a volume to the same disks as existing healthy replicas.

When this setting is un-checked, Longhorn Manager prevents scheduling new replicas of a volume to the same disks as existing healthy replicas.

[NOTE]
====
* Even when disk sharing is enabled, {longhorn-product-name} prefers to use a different disk when available, including disks on the same node.
* This setting is superseded if replicas are forbidden to share a zone or a node by either of the other Soft Anti-Affinity settings.
====

=== Replica Auto Balance

*Default value*: `disabled`

Enable this setting automatically rebalances replicas when discovered an available node.

The available global options are:

* `disabled`. This is the default option. No Replica Auto Balance will be done.
* `least-effort`. This option instructs {longhorn-product-name} to balance replicas for minimal redundancy.
* `best-effort`. This option instructs {longhorn-product-name} try to balancing replicas for even redundancy.
{longhorn-product-name} does not forcefully re-schedule the replicas to a zone that does not have enough nodes
to support even balance. Instead, {longhorn-product-name} will re-schedule to balance at the node level.

{longhorn-product-name} also supports customizing for individual volume. The setting can be specified in UI or with Kubernetes manifest volume.spec.replicaAutoBalance, this overrules the global setting.
The available volume spec options are:

*Default value*: `ignored`

* `ignored`. This is the default option that instructs {longhorn-product-name} to inherit from the global setting.
* `disabled`. This option instructs {longhorn-product-name} not to perform Replica Auto Balance."
* `least-effort`. This option instructs {longhorn-product-name} to balance replicas for minimal redundancy.
* `best-effort`. This option instructs {longhorn-product-name} to try balancing replicas for even redundancy.
{longhorn-product-name} does not forcefully re-schedule the replicas to a zone that does not have enough nodes
to support even balance. Instead, {longhorn-product-name} will re-schedule to balance at the node level.

=== Replica Auto Balance Disk Pressure Threshold (%)

*Default value*: `90`

Percentage of currently used storage that triggers automatic replica rebalancing.

When the threshold is reached, {longhorn-product-name} automatically rebuilds replicas that are under disk pressure on another disk within the same node.

To disable this setting, set the value to *0*.

This setting takes effect only when the following conditions are met:

* <<_replica_auto_balance,Replica Auto Balance>> is set to *best-effort*. To disable this setting (disk pressure threshold) when Replica Auto Balance is set to best-effort, set the value of this setting to *0*.
* At least one other disk on the node has sufficient available space.

This setting is not affected by <<_replica_node_level_soft_anti_affinity,Replica Node Level Soft Anti_Affinity>>, which can prevent {longhorn-product-name} from rebuilding a replica on the same node. Regardless of that setting's value, this setting still allows {longhorn-product-name} to attempt replica rebuilding on a different disk on the same node for migration purposes.

=== Storage Minimal Available Percentage

*Default value*: `25`

This setting controls the minimum free space that must remain on a disk, based on its *Storage Maximum*, before {longhorn-product-name} can schedule a new replica.

By default, {longhorn-product-name} ensures that at least *25%* of the disk's total capacity remains free. If adding a replica would reduce the available space below this limit, {longhorn-product-name} temporarily marks the disk as unavailable for scheduling until sufficient space is freed.

This safeguard helps protect your disks from becoming too full, which can cause performance issues or storage failures. Maintaining a buffer of free space helps keep the system stable and ensures room for unexpected storage needs.

See xref:nodes/multiple-disks#_configuration[Multiple Disks Support] for details.

=== Storage Over Provisioning Percentage

*Default value*: `100`

The over-provisioning percentage defines the amount of storage that can be allocated relative to the hard drive's capacity.

Adjusting this setting allows the Longhorn Manager to schedule new replicas on a disk as long as the combined size of all replicas remains within the permitted over-provisioning percentage of the usable disk space. The usable disk space is calculated as *Storage Maximum* minus *Storage Reserved*.

[NOTE]
====
Replicas might consume more space than a volume's nominal size due to snapshot data. To reclaim disk space, delete snapshots that are no longer needed.
====

[Example]
====
Suppose a disk has a *Storage Maximum* of 100 GiB and *Storage Reserved* of 10 GiB, resulting in 90 GiB of usable capacity.

If the Storage Over-Provisioning Percentage is set to 200%, the maximum allowed Storage Scheduled is 180 GiB (200% of 90 GiB).

This means the Longhorn Manager can continue scheduling replicas to this disk until the total scheduled size reaches 180 GiB, even though the actual usable space is only 90 GiB.
====

=== Storage Reserved Percentage For Default Disk

*Default value*: `30`

The reserved percentage specifies the percentage of disk space that will not be allocated to the default disk on each new Longhorn node.

This setting only affects the default disk of a new adding node or nodes when installing {longhorn-product-name}.

=== Allow Empty Node Selector Volume

*Default value*: `true`

This setting allows replica of the volume without node selector to be scheduled on node with tags.

=== Allow Empty Disk Selector Volume

*Default value*: `true`

This setting allows replica of the volume without disk selector to be scheduled on disk with tags.

== Danger Zone

Starting with {longhorn-product-name} v1.6.0, {longhorn-product-name} allows you to modify the Danger Zone settings without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:

* No attached volumes: When no volumes are attached before the settings are configured, the setting changes are immediately applied.
* Engine image upgrade (live upgrade): During a live upgrade, which involves creating a new Instance Manager pod, the setting changes are immediately applied to the new pod.

Settings are synchronized hourly. When all volumes are detached, the settings in the following table are immediately applied and the system-managed components (for example, Instance Manager, CSI Driver, and engine images) are restarted.

If you do not detach all volumes before the settings are synchronized, the settings are not applied and you must reconfigure the same settings after detaching the remaining volumes. You can view the list of unapplied settings in the *Danger Zone* section of the {longhorn-product-name} UI, or run the following CLI command to check the value of the field `APPLIED`.

[subs="+attributes",shell]
----
  ~# kubectl -n longhorn-system get setting priority-class
  NAME             VALUE               APPLIED   AGE
  priority-class   longhorn-critical   true      3h26m
----

|===
| Setting | Additional Information | Affected Components

| <<_kubernetes_taint_toleration,Kubernetes Taint Toleration>>
| xref:nodes/taints-tolerations.adoc[Taints and Tolerations]
| System-managed components

| <<_priority_class,Priority Class>>
| xref:nodes/priority-class.adoc[Priority Class]
| System-managed components

| <<_system_managed_components_node_selector,System Managed Components Node Selector>>
| xref:nodes/node-selector.adoc[Node Selector]
| System-managed components

| <<_storage_network,Storage Network>>
| xref:longhorn-system/networking/storage-network.adoc[Storage Network]
| Instance Manager and Backing Image components

| <<_v1_data_engine,V1 Data Engine>>
|
| Instance Manager component

| <<_v2_data_engine,V2 Data Engine>>
| xref:longhorn-system/v2-data-engine/quick-start-guide.adoc[V2 Data Engine (Experimental)]
| Instance Manager component

| <<_guaranteed_instance_manager_cpu,Guaranteed Instance Manager CPU>>
|
| Instance Manager component
|===

For V1 and V2 Data Engine settings, you can disable the Data Engines only when all associated volumes are detached. For example, you can disable the V2 Data Engine only when all V2 volumes are detached (even when V1 volumes are still attached).

=== V1 Data Engine

*Default value*: `true`

This setting lets you enable the V1 Data Engine.

=== V2 Data Engine

*Default value*: `false`

This setting enables the V2 Data Engine, which is an experimental feature based on the Storage Performance Development Kit (SPDK). The V2 Data Engine is an experimental feature and should not be used in production environments. For more information, see xref:longhorn-system/v2-data-engine/v2-data-engine.adoc[V2 Data Engine (Experimental)].

[WARNING]
====
* DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. {longhorn-product-name} will block this setting update when there are attached volumes.
* When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the Storage Performance Development Kit (SPDK) target daemon running within each instance-manager pod. The SPDK target daemon handles the input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
====

=== Concurrent Replica Rebuild Per Node Limit

*Default value*: `5`

This setting controls how many replicas on a node can be rebuilt simultaneously.

Typically, {longhorn-product-name} can block the replica starting once the current rebuilding count on a node exceeds the limit. But when the value is 0, it means disabling the replica rebuilding.

[WARNING]
====
* The old setting "Disable Replica Rebuild" is replaced by this setting.
* Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
* When the value is 0, the eviction and data locality feature will not work. But this should not have any impact to any current replica rebuild and backup restore.
====

=== Concurrent Backing Image Replenish Per Node Limit

*Default value*: `5`

This setting controls how many backing image copies on a node can be replenished simultaneously.

Typically, {longhorn-product-name} can block the backing image copy starting once the current replenishing count on a node exceeds the limit. But when the value is 0, it means disabling the backing image replenish.

=== Kubernetes Taint Toleration

*Example*: `nodetype=storage:NoSchedule`

If you want to dedicate nodes to just store {longhorn-product-name} replicas and reject other general workloads, you can set tolerations for *all* {longhorn-product-name} components and add taints to the nodes dedicated for storage.

Longhorn system contains user deployed components (for example, Longhorn manager, Longhorn driver, Longhorn UI) and system managed components (for example, instance manager, engine image, CSI driver, etc.)
This setting only sets taint tolerations for system managed components.
Depending on how you deployed Longhorn, you need to set taint tolerations for user deployed components in Helm chart or deployment YAML file.

To apply the modified toleration setting immediately, ensure that all Longhorn volumes are detached. When volumes are in use, {longhorn-product-name} components are not restarted, and you need to reconfigure the settings after detaching the remaining volumes. Otherwise, you can wait for the setting change to be reconciled in an hour.
We recommend setting tolerations during {longhorn-product-name} deployment because the Longhorn system cannot be operated during the update.

Multiple tolerations can be set here, and these tolerations are separated by semicolon. For example:

* `key1=value1:NoSchedule; key2:NoExecute`
* `:` this toleration tolerates everything because an empty key with operator `Exists` matches all keys, values and effects
* `key1=value1:`  this toleration has empty effect. It matches all effects with key `key1`
See xref:nodes/taints-tolerations.adoc[Taint Toleration] for details.

=== Priority Class

*Default value*: `longhorn-critical`

By default, {longhorn-product-name} workloads run with the same priority as other pods in the cluster.  
When a node is under pressure, such as running out of memory, {longhorn-product-name} workloads are treated the same as other pods for eviction.

The Priority Class setting assigns a PriorityClass to {longhorn-product-name} system workloads.  
You can use this setting to give these workloads a higher priority so they are less likely to be evicted when a node experiences resource pressure.

The {longhorn-product-name} system includes user-deployed components (such as the Longhorn manager, Longhorn driver, and Longhorn UI) and system-managed components (such as the instance manager, engine image, and CSI driver).

This setting applies only to system-managed components. Depending on how you deployed {longhorn-product-name}, you must set the PriorityClass for user-deployed components in the Helm chart or in your deployment YAML.

[WARNING]
====
Change this setting only after detaching all Longhorn volumes.  
Applying the new PriorityClass restarts the Longhorn system components.  
During the update, the system is unavailable, and no Longhorn operations can run.  
To avoid disruption, configure the PriorityClass during the initial {longhorn-product-name} deployment.
====

See xref:nodes/priority-class.adoc[Priority Class] for details.

=== System Managed Components Node Selector

*Example*: `label-key1:label-value1;label-key2:label-value2`

To restrict {longhorn-product-name} components to only run on a particular set of nodes, you can set node selector for all {longhorn-product-name} components.

Longhorn system contains user deployed components (for example, Longhorn manager, Longhorn driver, Longhorn UI) and system managed components (for example, instance manager, engine image, CSI driver, etc.)
You need to set node selector for both. This setting only sets node selector for system managed components. Follow the instruction at xref:nodes/node-selector.adoc[Node Selector] to change node selector.

[WARNING]
====
Since all {longhorn-product-name} components would be restarted, the Longhorn system is unavailable temporarily.
====

To apply a setting immediately, ensure that all Longhorn volumes are detached. When volumes are in use, {longhorn-product-name} components are not restarted, and you need to reconfigure the settings after detaching the remaining volumes. Otherwise, you can wait for the setting change to be reconciled in an hour.
Do not operate the Longhorn system while node selector settings are updated and {longhorn-product-name} components are being restarted.

=== Kubernetes Cluster Autoscaler Enabled (Experimental)

*Default value*: `false`

Setting the Kubernetes Cluster Autoscaler Enabled to `true` allows {longhorn-product-name} to unblock the Kubernetes Cluster Autoscaler scaling.

See xref:high-availability/kubernetes-cluster-autoscaler.adoc[Kubernetes Cluster Autoscaler Support] for details.

WARNING: Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.

=== Storage Network

*Example*: `kube-system/demo-192-168-0-0`

The storage network uses Multus NetworkAttachmentDefinition to segregate the in-cluster data traffic from the default Kubernetes cluster network.

By default, this setting applies only to Longhorn's data-plan traffic path. For RWX (Read-Write-Many) volume endpoint traffic, see  <<_endpoint_network_for_rwx_volume,Endpoint Network for RWX Volume>>.

WARNING: This setting should change after all Longhorn volumes are detached because some pods that run Longhorn system components are recreated to apply the setting. When all volumes are detached, {longhorn-product-name} attempts to restart all Instance Manager and Backing Image Manager pods immediately. When volumes are in use, {longhorn-product-name} components are not restarted, and you need to reconfigure the settings after detaching the remaining volumes; otherwise, you can wait for the setting change to be reconciled in an hour.

See xref:longhorn-system/networking/storage-network.adoc[Storage Network] for details.

=== Endpoint Network for RWX Volume

*Default value*: `kube-system/demo-172-16-0-0`

Specify a Multus NetworkAttachmentDefinition to provide a dedicated network for mounting RWX (ReadWriteMany) volumes.

Leave this blank to use the default Kubernetes cluster network.

[WARNING]
====
This setting should change after all {longhorn-product-name} RWX volumes are detached because some pods that run {longhorn-product-name} components are recreated to apply the setting. When all RWX volumes are detached, {longhorn-product-name} attempts to restart all CSI plug-in pods immediately. When volumes are in use, pods that run {longhorn-product-name} components are not restarted, so the settings must be reconfigured after the remaining volumes are detached. If you are unable to manually reconfigure the settings, you can opt to wait because settings are synchronized hourly.

The RWX volumes are mounted with the endpoint network within the CSI plug-in pod container network namespace. As a result, restarting the CSI plug-in pod may lead to unresponsive RWX volume mounts. When this occurs, you must restart the workload pod to re-establish the mount connection. Alternatively, you can enable the <<_automatically_delete_workload_pod_when_the_volume_is_detached_unexpectedly,Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly>> setting.
====

For more information, see xref:longhorn-system/networking/storage-network.adoc[Storage Network].

=== Remove Snapshots During Filesystem Trim

*Example*: `false`

This setting allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children.

Since Longhorn filesystem trim feature can be applied to the volume head and the followed continuous removed or system snapshots only.

Notice that trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.

See xref:volumes/trim-filesystem.adoc[Trim Filesystem] for details.

=== Guaranteed Instance Manager CPU

*Default value*: `{"v1":"12","v2":"12"}`

Percentage of the total allocatable CPU resources on each node to reserve for each instance manager pod. For example, a value of `10` means 10% of the total CPU on a node will be allocated to each instance manager pod on that node. This helps maintain engine and replica stability during periods of high node workload.

In order to prevent unexpected volume instance (engine/replica) crash as well as guarantee a relative acceptable IO performance, you can use the following formula to calculate a value for this setting:

 Guaranteed Instance Manager CPU = The estimated max Longhorn volume engine and replica count on a node * 0.1 / The total allocatable CPUs on the node * 100.

The result of above calculation does not mean that is the maximum CPU resources the {longhorn-product-name} workloads require. To fully exploit the Longhorn volume input or output performance, you can allocate or guarantee more CPU resources via this setting.

If it is hard to estimate the usage now, you can leave it with the default value, which is 12%. Then you can tune it when there is no running workload using Longhorn volumes.

[WARNING]
====
* Value 0 means unsetting CPU requests for the instance manager pods.
* Considering the possible new instance manager pods in the further system upgrade, this float value ranges from 0 to 40.
* One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that {longhorn-product-name} can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
* This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
* For the v2 Data Engine, the Storage Performance Development Kit (SPDK) target daemon inside each instance manager pod uses one or more dedicated CPU cores. Setting a minimum CPU usage is critical to maintaining stability during periods of high node load.
====

=== Disable Snapshot Purge

*Default value*: `false`

When set to true, temporarily prevent all attempts to purge volume snapshots.

{longhorn-product-name} typically purges snapshots during replica rebuilding and user-initiated snapshot deletion. While purging,
{longhorn-product-name} coalesces unnecessary snapshots into their newer counterparts, freeing space consumed by historical data.

Allowing snapshot purging during normal operations is ideal, but this process temporarily consumes additional disk
space. If insufficient disk space prevents the process from continuing, consider temporarily disabling purging while data is moved to other disks.

=== Auto Cleanup Snapshot When Delete Backup

*Default value*: `false`

When set to true, the snapshot used by the backup will be automatically cleaned up when the backup is deleted.

=== Instance Manager Pod Liveness Probe Timeout

*Default value*: `10` (in seconds)

The setting specifies the timeout for the instance manager pod liveness probe. The default value is 10 seconds.

[WARNING]
====
When applying the setting, {longhorn-product-name} will try to restart all instance-manager pods if all volumes are detached and eventually restart the instance manager pod without instances running on the instance manager.
====

=== Data Engine CPU Mask

*Default value*: `{"v2":"true"}`

This setting applies only to the V2 Data Engine. It enables hugepages for the Storage Performance Development Kit (SPDK) target daemon. If this setting is disabled, legacy memory is used. The allocation size for this memory is set through the Data Engine Memory Size setting.

=== Data Engine Hugepage Enabled

*Default value*: `{"v2":"2048"}` 

This setting applies only to the V2 Data Engine and specifies the memory size, in MiB, allocated to the Storage Performance Development Kit (SPDK) target daemon.

* When Hugepage is enabled, this value defines the Hugepage size.
* When Hugepage is disabled, legacy memory is used.

=== Data Engine Interrupt Mode Enabled

*Default value*: `{"v2":"false"}`

It applies only to the *V2 Data Engine*. It controls whether the Storage Performance Development Kit (SPDK) target daemon runs in *interrupt mode* or the default *polling mode*.

* `true`: It enables interrupt mode, reducing CPU usage by handling I/O through interrupts.
* `false`: It keeps polling mode enabled for maximum performance and lowest latency.

[WARNING]
====
DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. {longhorn-product-name} will block this setting update when there are attached v2 volumes.
====

=== Log Path

*Default value*: `/var/lib/longhorn/logs/`

Specifies the directory on the host where {longhorn-product-name} stores log files for the instance manager pod. Currently, it is only used for the instance manager pods in the v2 data engine.

=== Snapshot Heavy Task Concurrent Limit

*Default value*: `5`

* `< 1`: unlimited concurrent heavy snapshot tasks.

This setting controls how many snapshot-heavy tasks, such as purge and clone operations, can run concurrently on each node. It is a best-effort mechanism. Because the system is distributed, temporary oversubscription can occur. The limiter reduces worst-case overload but does not guarantee perfect enforcement.

=== System Managed CSI Components Resource Limits

*Default value*: `""`

This setting configures CPU and memory requests and limits for system-managed CSI components.  
Supported components include:`csi-attacher`, `csi-provisioner`, `csi-resizer`, `csi-snapshotter`, `longhorn-csi-plugin`, `node-driver-registrar`, and `longhorn-liveness-probe`.

The value must be a JSON object with component names as keys and Kubernetes `ResourceRequirements` (`requests` and `limits`) as values. Only the components defined in the JSON object have their resource requirements overridden. All other components continue to use the {longhorn-product-name} default values.

[NOTE]
====
Updating resource limits restarts the affected CSI components. During the restart, volume provisioning, expansion, snapshot, and attach or detach operations may be delayed. Existing mounted volumes remain available.
====

*Example*
[source,json]
----
{
  "csi-attacher": {
    "requests": {"cpu": "100m", "memory": "128Mi"},
    "limits": {"cpu": "200m", "memory": "256Mi"}
  },
  "csi-provisioner": {
    "requests": {"cpu": "100m", "memory": "128Mi"},
    "limits": {"cpu": "200m", "memory": "256Mi"}
  },
  "longhorn-csi-plugin": {
    "requests": {"cpu": "100m", "memory": "128Mi"},
    "limits": {"cpu": "200m", "memory": "256Mi"}
  },
  "node-driver-registrar": {
    "requests": {"cpu": "50m", "memory": "64Mi"},
    "limits": {"cpu": "100m", "memory": "128Mi"}
  }
}
----
